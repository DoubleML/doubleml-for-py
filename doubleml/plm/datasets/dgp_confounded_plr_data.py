import numpy as np
from scipy.linalg import toeplitz
from scipy.optimize import minimize_scalar


def make_confounded_plr_data(n_obs=500, theta=5.0, cf_y=0.04, cf_d=0.04, **kwargs):
    """
    Generates counfounded data from an partially linear regression model.

    The data generating process is defined as follows (similar to the Monte Carlo simulation used
    in Sant'Anna and Zhao (2020)). Let :math:`X= (X_1, X_2, X_3, X_4, X_5)^T \\sim \\mathcal{N}(0, \\Sigma)`,
    where  :math:`\\Sigma` is a matrix with entries
    :math:`\\Sigma_{kj} = c^{|j-k|}`. The default value is  :math:`c = 0`, corresponding to the identity matrix.
    Further, define :math:`Z_j = (\\tilde{Z_j} - \\mathbb{E}[\\tilde{Z}_j]) / \\sqrt{\\text{Var}(\\tilde{Z}_j)}`,
    where

    .. math::

        \\tilde{Z}_1 &= \\exp(0.5 \\cdot X_1)

        \\tilde{Z}_2 &= 10 + X_2/(1 + \\exp(X_1))

        \\tilde{Z}_3 &= (0.6 + X_1 \\cdot X_3 / 25)^3

        \\tilde{Z}_4 &= (20 + X_2 + X_4)^2.

    Additionally, generate a confounder :math:`A \\sim \\mathcal{U}[-1, 1]`.
    At first, define the treatment as

    .. math::

        D = -Z_1 + 0.5 \\cdot Z_2 - 0.25 \\cdot Z_3 - 0.1 \\cdot Z_4 + \\gamma_A \\cdot A + \\varepsilon_D

    and with :math:`\\varepsilon \\sim \\mathcal{N}(0,1)`.
    Since :math:`A` is independent of :math:`X`, the long and short form of the treatment regression are given as

    .. math::

        E[D|X,A] = -Z_1 + 0.5 \\cdot Z_2 - 0.25 \\cdot Z_3 - 0.1 \\cdot Z_4 + \\gamma_A \\cdot A

        E[D|X] = -Z_1 + 0.5 \\cdot Z_2 - 0.25 \\cdot Z_3 - 0.1 \\cdot Z_4.

    Further, generate the outcome of interest :math:`Y` as

    .. math::

        Y &= \\theta \\cdot D + g(Z) + \\beta_A \\cdot A + \\varepsilon

        g(Z) &= 210 + 27.4 \\cdot Z_1 +13.7 \\cdot (Z_2 + Z_3 + Z_4)

    where :math:`\\varepsilon \\sim \\mathcal{N}(0,5)`.
    This implies an average treatment effect of :math:`\\theta`. Additionally, the long and short forms of
    the conditional expectation take the following forms

    .. math::

        \\mathbb{E}[Y|D, X, A] &= \\theta \\cdot D + g(Z) + \\beta_A \\cdot A

        \\mathbb{E}[Y|D, X] &= (\\theta + \\gamma_A\\beta_A \\frac{\\mathrm{Var}(A)}{\\mathrm{Var}(D)}) \\cdot D + g(Z).

    Consequently, the strength of confounding is determined via :math:`\\gamma_A` and :math:`\\beta_A`.
    Both are chosen to obtain the desired confounding of the outcome and Riesz Representer (in sample).

    The observed data is given as :math:`W = (Y, D, X)`.
    Further, orcale values of the confounder :math:`A`, the transformed covariated :math:`Z`, the effect :math:`\\theta`,
    the coefficients :math:`\\gamma_a`, :math:`\\beta_a`, the long and short forms of the main regression and
    the propensity score are returned in a dictionary.

    Parameters
    ----------
    n_obs : int
        The number of observations to simulate.
        Default is ``500``.
    theta : float or int
        Average treatment effect.
        Default is ``5.0``.
    cf_y : float
        Percentage of the residual variation of the outcome explained by latent/confounding variable.
        Default is ``0.04``.
    cf_d : float
        Percentage gains in the variation of the Riesz Representer generated by latent/confounding variable.
        Default is ``0.04``.

    Returns
    -------
    res_dict : dictionary
       Dictionary with entries ``x``, ``y``, ``d`` and ``oracle_values``.

    References
    ----------
    Sant'Anna, P. H. and Zhao, J. (2020),
    Doubly robust difference-in-differences estimators. Journal of Econometrics, 219(1), 101-122.
    doi:`10.1016/j.jeconom.2020.06.003 <https://doi.org/10.1016/j.jeconom.2020.06.003>`_.
    """
    c = kwargs.get("c", 0.0)
    dim_x = kwargs.get("dim_x", 4)

    # observed covariates
    cov_mat = toeplitz([np.power(c, k) for k in range(dim_x)])
    x = np.random.multivariate_normal(
        np.zeros(dim_x),
        cov_mat,
        size=[
            n_obs,
        ],
    )

    z_tilde_1 = np.exp(0.5 * x[:, 0])
    z_tilde_2 = 10 + x[:, 1] / (1 + np.exp(x[:, 0]))
    z_tilde_3 = (0.6 + x[:, 0] * x[:, 2] / 25) ** 3
    z_tilde_4 = (20 + x[:, 1] + x[:, 3]) ** 2

    z_tilde = np.column_stack((z_tilde_1, z_tilde_2, z_tilde_3, z_tilde_4, x[:, 4:]))
    z = (z_tilde - np.mean(z_tilde, axis=0)) / np.std(z_tilde, axis=0)

    # error terms
    var_eps_y = 5
    eps_y = np.random.normal(loc=0, scale=np.sqrt(var_eps_y), size=n_obs)
    var_eps_d = 1
    eps_d = np.random.normal(loc=0, scale=np.sqrt(var_eps_d), size=n_obs)

    # unobserved confounder
    a_bounds = (-1, 1)
    a = np.random.uniform(low=a_bounds[0], high=a_bounds[1], size=n_obs)
    var_a = np.square(a_bounds[1] - a_bounds[0]) / 12

    # get the required impact of the confounder on the propensity score
    m_short = -z[:, 0] + 0.5 * z[:, 1] - 0.25 * z[:, 2] - 0.1 * z[:, 3]

    def f_m(gamma_a):
        rr_long = eps_d / var_eps_d
        rr_short = (gamma_a * a + eps_d) / (gamma_a**2 * var_a + var_eps_d)
        C2_D = (np.mean(np.square(rr_long)) - np.mean(np.square(rr_short))) / np.mean(np.square(rr_short))
        return np.square(C2_D / (1 + C2_D) - cf_d)

    gamma_a = minimize_scalar(f_m).x
    m_long = m_short + gamma_a * a
    d = m_long + eps_d

    # short and long version of g
    g_partial_reg = 210 + 27.4 * z[:, 0] + 13.7 * (z[:, 1] + z[:, 2] + z[:, 3])

    var_d = np.var(d)

    def f_g(beta_a):
        g_diff = beta_a * (a - gamma_a * (var_a / var_d) * d)
        y_diff = eps_y + g_diff
        return np.square(np.mean(np.square(g_diff)) / np.mean(np.square(y_diff)) - cf_y)

    beta_a = minimize_scalar(f_g).x

    g_long = theta * d + g_partial_reg + beta_a * a
    g_short = (theta + gamma_a * beta_a * var_a / var_d) * d + g_partial_reg

    y = g_long + eps_y

    oracle_values = {
        "g_long": g_long,
        "g_short": g_short,
        "m_long": m_long,
        "m_short": m_short,
        "theta": theta,
        "gamma_a": gamma_a,
        "beta_a": beta_a,
        "a": a,
        "z": z,
    }

    res_dict = {"x": x, "y": y, "d": d, "oracle_values": oracle_values}

    return res_dict
